{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/matt/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import numpy.random as rng\n",
    "import pandas as pd\n",
    "import os, pdb, re\n",
    "import string\n",
    "import stats\n",
    "import keras.preprocessing.text as text\n",
    "from keras.preprocessing import sequence\n",
    "from keras import utils\n",
    "float_formatter = lambda x: \"%.2f\" % x\n",
    "np.set_printoptions(linewidth=200,threshold=np.nan,formatter={'float_kind':float_formatter})\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "pd.set_option(\"display.max_colwidth\",200)\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "#Preprocessing\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_WORD_COUNT = 60\n",
    "N_OUT = 5\n",
    "BATCH_SIZE = 5\n",
    "HIDDEN_SIZE = 8\n",
    "NUM_LAYERS = 2\n",
    "INIT_SCALE = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fx = lambda x: pd.read_csv(x,delimiter=\"\\t\",header=0)\n",
    "train_df,test_df = map(fx, [\"train.tsv\",\"test.tsv\"])\n",
    "feat_names = train_df.columns.values\n",
    "#train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get full sentences from data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def group_max(group):\n",
    "    idx = np.where(group[\"Phrase\"].apply(len)==group[\"Phrase\"].apply(len).max())[0][0]\n",
    "    return group[\"Phrase\"].iloc[idx]\n",
    "df = train_df.groupby(\"SentenceId\").apply(group_max).values\n",
    "#remove_stop = lambda sentence: [word for word in sentence if word not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Love Song of J. Alfred Prufrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_table(\"The Love Song of J. Alfred Prufrock.txt\",header=None,names=[\"Line\"],dtype={\"Line\":str})\n",
    "txt = df[\"Line\"].values\n",
    "txt = ' \\n '.join([''.join(row) for row in txt]).lower()\n",
    "txt = re.sub(r'[^\\w\\s]',' ',txt)\n",
    "df_clean = txt.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_clean_yellow = list(filter(lambda row: \"yellow\" in row,df_clean))\n",
    "df_clean_will = list(filter(lambda row: \"will\" in row,df_clean))\n",
    "df_clean = np.concatenate((df_clean_will,df_clean_yellow))\n",
    "#df_clean = df_clean_yellow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Data_obj():\n",
    "    def __init__(self,batch_size,clean_data):\n",
    "        self.epoch = 1\n",
    "        self.i = self.k = 0\n",
    "        self.batch_size = batch_size\n",
    "        self.clean_data = clean_data\n",
    "        \n",
    "        self.Tokenizer = text.Tokenizer()\n",
    "        self.Tokenizer.fit_on_texts(self.clean_data)\n",
    "        self.words = self.Tokenizer.word_index.keys()\n",
    "        self.encoded_text = self.Tokenizer.texts_to_sequences(self.clean_data)\n",
    "        \n",
    "        self.inverse_tokenizer = lambda num: list(self.Tokenizer.word_index.keys())[list(self.Tokenizer.word_index.values()).index(num)] #inverse\n",
    "        self.inverse_tokenizer_sentence = lambda sentence: list(map(self.inverse_tokenizer,sentence))\n",
    "        \n",
    "        self.vocab_size = len(self.Tokenizer.word_index) + 1\n",
    "        print(\"There are {0} unique words in data set.\".format(self.vocab_size))\n",
    "        \n",
    "    def shuffle(self):\n",
    "        rng.shuffle(self.encoded_text)\n",
    "        \n",
    "    def new_batch(self):\n",
    "        return np.zeros((self.batch_size,2)).astype(np.int32)\n",
    "    \n",
    "    def generator(self):\n",
    "        self.shuffle()\n",
    "        batch = self.new_batch()\n",
    "        self.total_examples_seen = 0\n",
    "        while True:\n",
    "            \n",
    "            self.current_sentence = self.encoded_text[self.i]\n",
    "            sentence_len = len(self.current_sentence)\n",
    "            if sentence_len < 2:\n",
    "                self.i += 1\n",
    "                continue\n",
    "            for j in range(sentence_len):\n",
    "                context = self.current_sentence[j]\n",
    "\n",
    "                if j == 0:\n",
    "                    target = self.current_sentence[j+1]\n",
    "                elif j == sentence_len - 1:\n",
    "                    target = self.current_sentence[j-1]\n",
    "                elif rng.uniform() < 0.5:\n",
    "                    target = self.current_sentence[j-1]\n",
    "                else:\n",
    "                    target = self.current_sentence[j+1]\n",
    "\n",
    "                batch[self.k,0] = context\n",
    "                batch[self.k,1] = target\n",
    "                if self.k == BATCH_SIZE - 1:\n",
    "                    self.k = 0\n",
    "                    yield batch\n",
    "                    batch = self.new_batch()\n",
    "                    self.total_examples_seen += self.batch_size\n",
    "                else:\n",
    "                    self.k += 1\n",
    "            self.i+=1\n",
    "            if self.i == len(self.encoded_text):\n",
    "                self.epoch += 1\n",
    "                self.i = 0\n",
    "                self.shuffle()  #shuffle after epoch\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 56 unique words in data set.\n"
     ]
    }
   ],
   "source": [
    "data_obj = Data_obj(batch_size=BATCH_SIZE,clean_data=df_clean)\n",
    "generate_batch = data_obj.generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 1, 5, 6, 4, 1, 5, 6] ['there', 'will', 'be', 'time', 'there', 'will', 'be', 'time']\n",
      "[4 1] ['there', 'will']\n",
      "[1 4] ['will', 'there']\n",
      "[5 1] ['be', 'will']\n",
      "[6 4] ['time', 'there']\n",
      "[4 1] ['there', 'will']\n",
      "[16, 32, 3, 33, 34, 35, 36, 1, 37] ['for', 'decisions', 'and', 'revisions', 'which', 'a', 'minute', 'will', 'reverse']\n",
      "[1 5] ['will', 'be']\n",
      "[5 1] ['be', 'will']\n",
      "[6 5] ['time', 'be']\n",
      "[16 32] ['for', 'decisions']\n",
      "[32  3] ['decisions', 'and']\n",
      "[16, 32, 3, 33, 34, 35, 36, 1, 37] ['for', 'decisions', 'and', 'revisions', 'which', 'a', 'minute', 'will', 'reverse']\n",
      "[ 3 33] ['and', 'revisions']\n",
      "[33 34] ['revisions', 'which']\n",
      "[34 33] ['which', 'revisions']\n",
      "[35 34] ['a', 'which']\n",
      "[36 35] ['minute', 'a']\n",
      "[8, 1, 12, 13, 14, 25, 26, 27, 15] ['they', 'will', 'say', 'how', 'his', 'hair', 'is', 'growing', 'thin']\n",
      "[ 1 37] ['will', 'reverse']\n",
      "[37  1] ['reverse', 'will']\n",
      "[8 1] ['they', 'will']\n",
      "[ 1 12] ['will', 'say']\n",
      "[12  1] ['say', 'will']\n",
      "[8, 1, 12, 13, 14, 25, 26, 27, 15] ['they', 'will', 'say', 'how', 'his', 'hair', 'is', 'growing', 'thin']\n",
      "[13 12] ['how', 'say']\n",
      "[14 13] ['his', 'how']\n",
      "[25 14] ['hair', 'his']\n",
      "[26 25] ['is', 'hair']\n",
      "[27 15] ['growing', 'thin']\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    data = next(generate_batch)\n",
    "    print(data_obj.current_sentence,data_obj.inverse_tokenizer_sentence(data_obj.current_sentence))\n",
    "    for i in data:\n",
    "        print(i,data_obj.inverse_tokenizer_sentence(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding layer - Turns positive integers (indexes) into dense vectors of fixed sizeâ€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocabulary_size = data_obj.vocab_size\n",
    "embedding_size = HIDDEN_SIZE\n",
    "train_inputs = tf.placeholder(tf.int32, shape=[None])\n",
    "#train_labels = tf.placeholder(tf.int32, shape=[None, 1])\n",
    "train_context = tf.placeholder(tf.int32, shape=[None, 1])\n",
    "embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "embed = tf.nn.embedding_lookup(embeddings, train_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nce_weights = tf.Variable(\n",
    "        tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                            stddev=1.0 / np.sqrt(embedding_size)))\n",
    "nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "hidden_out = tf.matmul(embed, tf.transpose(nce_weights)) + nce_biases\n",
    "soft_max = tf.nn.softmax(hidden_out)\n",
    "loss = tf.reduce_mean(\n",
    "        tf.nn.nce_loss(weights=nce_weights,\n",
    "                       biases=nce_biases,\n",
    "                       labels=train_context,\n",
    "                       inputs=embed,\n",
    "                       num_sampled=1,\n",
    "                       num_classes=vocabulary_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learning_rate = tf.placeholder(tf.float32)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "saver = tf.train.Saver()\n",
    "model_path = \"/tmp/model.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 56 unique words in data set.\n",
      "0 seen with running loss of 5.552. Current epoch = 1. Current lr = 0.300\n",
      "2000 seen with running loss of 2.543. Current epoch = 19. Current lr = 0.300\n",
      "4000 seen with running loss of 1.522. Current epoch = 38. Current lr = 0.300\n",
      "6000 seen with running loss of 1.156. Current epoch = 57. Current lr = 0.300\n",
      "8000 seen with running loss of 0.812. Current epoch = 76. Current lr = 0.300\n",
      "10000 seen with running loss of 0.643. Current epoch = 95. Current lr = 0.300\n",
      "12000 seen with running loss of 0.637. Current epoch = 114. Current lr = 0.300\n",
      "14000 seen with running loss of 0.546. Current epoch = 133. Current lr = 0.300\n",
      "16000 seen with running loss of 0.488. Current epoch = 151. Current lr = 0.300\n",
      "18000 seen with running loss of 0.491. Current epoch = 170. Current lr = 0.300\n",
      "20000 seen with running loss of 0.462. Current epoch = 189. Current lr = 0.300\n",
      "22000 seen with running loss of 0.377. Current epoch = 208. Current lr = 0.300\n",
      "24000 seen with running loss of 0.391. Current epoch = 227. Current lr = 0.300\n",
      "26000 seen with running loss of 0.392. Current epoch = 246. Current lr = 0.300\n",
      "28000 seen with running loss of 0.381. Current epoch = 265. Current lr = 0.300\n",
      "30000 seen with running loss of 0.385. Current epoch = 284. Current lr = 0.300\n",
      "32000 seen with running loss of 0.362. Current epoch = 302. Current lr = 0.300\n",
      "34000 seen with running loss of 0.390. Current epoch = 321. Current lr = 0.300\n",
      "36000 seen with running loss of 0.386. Current epoch = 340. Current lr = 0.300\n",
      "38000 seen with running loss of 0.387. Current epoch = 359. Current lr = 0.300\n",
      "40000 seen with running loss of 0.377. Current epoch = 378. Current lr = 0.300\n",
      "42000 seen with running loss of 0.379. Current epoch = 397. Current lr = 0.300\n",
      "44000 seen with running loss of 0.319. Current epoch = 416. Current lr = 0.300\n",
      "46000 seen with running loss of 0.344. Current epoch = 435. Current lr = 0.300\n",
      "48000 seen with running loss of 0.364. Current epoch = 453. Current lr = 0.300\n",
      "50000 seen with running loss of 0.334. Current epoch = 472. Current lr = 0.300\n",
      "52000 seen with running loss of 0.343. Current epoch = 491. Current lr = 0.300\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    embeddings_before = embeddings.eval()\n",
    "    data_obj = Data_obj(batch_size=BATCH_SIZE,clean_data=df_clean)\n",
    "    generate_batch = data_obj.generator()\n",
    "    cur_losses = []\n",
    "    lr = 0.3\n",
    "    while True:\n",
    "        data = next(generate_batch)\n",
    "        feed_dict = {train_inputs: data[:,0],train_context:data[:,[1]],learning_rate:lr}\n",
    "        _, cur_loss = sess.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        cur_losses.append(cur_loss)\n",
    "        if data_obj.total_examples_seen % 2000 == 0:\n",
    "            print(\"{0} seen with running loss of {1:.3f}. Current epoch = {2}. Current lr = {3:.3f}\".format(data_obj.total_examples_seen,np.mean(cur_losses),data_obj.epoch,lr))\n",
    "            cur_losses = []\n",
    "            lr /= 1.0\n",
    "        if data_obj.epoch == 500:\n",
    "            print(\"Finished.\")\n",
    "            break\n",
    "    save_path = saver.save(sess,model_path )\n",
    "    learnt_embeddings = embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.27, -0.23, -0.09, 0.83, 0.05, -0.15, 0.77, -0.93],\n",
       "       [-1.41, -2.00, -1.02, 0.58, -2.03, -0.82, -0.99, 0.84],\n",
       "       [2.12, 0.81, -2.88, 1.05, 0.54, 1.68, -1.72, 1.43],\n",
       "       [-2.43, -0.47, 0.58, 1.12, 1.01, 0.67, -0.47, 3.85],\n",
       "       [-1.80, 1.23, 0.43, 0.89, 2.07, 0.07, -2.02, 1.54],\n",
       "       [-0.74, 0.40, 0.50, 1.01, 1.83, 0.06, -3.16, -0.15],\n",
       "       [0.59, 0.21, -2.73, -0.14, -2.60, 0.27, -2.62, 0.23],\n",
       "       [-1.69, 1.86, 0.77, 2.50, -0.58, -1.50, -1.79, 0.58],\n",
       "       [-0.79, 0.87, 1.38, -0.49, -0.06, 2.05, -3.03, 0.84],\n",
       "       [-0.99, 2.68, -0.41, 0.22, -2.49, -1.33, -1.61, 1.78],\n",
       "       [-1.27, 0.54, -2.20, 1.29, -1.27, -0.58, -2.49, -1.71],\n",
       "       [-1.18, -1.67, 0.04, 1.56, 1.48, -1.59, -2.57, 0.63],\n",
       "       [1.28, 1.04, 0.30, -0.03, 1.78, -0.25, -2.37, 2.20],\n",
       "       [-1.21, -0.89, -2.89, -0.93, 1.72, 1.01, -0.96, 2.04],\n",
       "       [1.25, -0.96, 1.09, 2.99, -0.27, -0.46, -2.38, 2.00],\n",
       "       [-0.27, -2.04, -2.23, 0.77, 0.94, 0.27, -3.13, 0.08],\n",
       "       [-1.84, 1.23, -0.65, -1.31, -1.05, 0.42, -1.47, 2.43],\n",
       "       [0.36, 2.08, -0.13, -0.76, 0.82, -0.74, -2.07, 1.62],\n",
       "       [0.11, -0.89, 0.67, -0.42, -1.18, -0.17, -3.86, 0.49],\n",
       "       [-1.90, 0.58, 0.80, 2.26, -1.86, 2.12, -1.89, 0.77],\n",
       "       [-2.15, 2.22, -1.89, 0.93, -1.26, 1.17, -0.56, 1.13],\n",
       "       [0.09, 0.34, -2.36, 3.51, 1.19, 0.95, -1.76, 0.41],\n",
       "       [0.77, -0.65, -0.64, -0.06, -0.76, 2.64, -2.69, 2.01],\n",
       "       [-0.42, 1.13, -1.87, -0.91, -0.49, 0.95, -2.27, -0.69],\n",
       "       [-1.16, 0.83, -2.18, 0.11, 0.38, 0.76, -2.26, -0.96],\n",
       "       [-1.09, -0.88, -1.02, 1.06, 1.17, 2.11, -0.91, 1.11],\n",
       "       [0.05, -0.94, -0.24, 1.92, -0.05, 1.23, -2.61, -0.75],\n",
       "       [0.08, 0.30, -0.07, 2.27, -0.19, 2.03, -0.96, 1.70],\n",
       "       [0.54, 0.15, -2.25, 1.84, -0.13, -1.61, -0.76, 2.79],\n",
       "       [-1.24, -0.10, -2.02, -0.60, 1.14, 1.36, -1.63, -0.67],\n",
       "       [-1.53, -0.14, -1.94, 0.18, 0.91, 0.01, -1.90, -0.76],\n",
       "       [-0.41, 0.68, -0.93, 2.65, 0.34, 1.86, -0.36, 1.37],\n",
       "       [-0.05, 1.93, -3.12, -0.09, 0.75, 0.02, -1.53, 0.29],\n",
       "       [0.57, 1.69, -1.52, 0.11, -0.92, -0.19, -1.77, -0.45],\n",
       "       [-2.21, 1.19, -1.62, 0.15, 0.78, -0.70, -0.34, 1.74],\n",
       "       [0.74, 0.34, -0.51, 0.09, -1.69, -0.43, -1.17, 1.85],\n",
       "       [-2.01, 1.90, -0.58, -0.66, 1.10, 0.05, -1.58, 0.84],\n",
       "       [-0.79, 2.19, 0.08, 1.04, 0.59, 0.45, -2.41, 0.38],\n",
       "       [-1.07, -0.20, -0.79, 1.39, 0.41, 1.38, -1.54, -0.64],\n",
       "       [0.99, -0.72, -0.47, 1.13, -0.04, -0.29, -1.85, -0.13],\n",
       "       [-1.07, -0.90, -0.93, -0.29, 0.82, 0.01, -1.72, -0.43],\n",
       "       [-0.18, 0.32, -0.94, 1.60, 0.52, -1.59, -1.50, -0.02],\n",
       "       [-0.75, -0.99, -0.54, -1.47, 0.29, 0.27, -2.35, 1.21],\n",
       "       [-1.06, -0.51, -0.86, -0.24, -0.26, -1.57, -1.44, 1.94],\n",
       "       [-1.10, -0.16, -0.85, 0.34, 0.06, -1.83, -0.74, 1.66],\n",
       "       [-0.07, -0.30, 0.69, -0.65, -0.90, 0.69, -1.77, 1.97],\n",
       "       [0.14, 1.87, -0.18, -1.45, 0.33, 0.57, -2.77, 0.69],\n",
       "       [0.43, 1.00, -1.54, -1.31, -1.01, -0.04, -2.65, 0.59],\n",
       "       [0.73, -0.45, -0.48, -0.28, -0.43, 2.84, -2.40, 1.14],\n",
       "       [1.69, -0.38, -0.72, 0.29, -0.01, -0.80, -2.85, 0.24],\n",
       "       [-0.56, 1.68, -0.35, 0.81, -2.27, 1.34, -1.02, 1.13],\n",
       "       [1.01, -0.22, 0.00, -0.14, -0.08, -0.28, -3.12, 0.39],\n",
       "       [-2.43, 0.66, -0.36, -0.43, -1.30, 0.91, -1.25, 0.57],\n",
       "       [-0.15, -1.06, -0.52, 0.07, -1.08, 1.79, -1.14, 1.60],\n",
       "       [-1.96, 1.38, 0.23, 0.59, -1.49, 0.14, -0.93, 0.53],\n",
       "       [-1.84, 1.38, -0.78, -0.39, -1.91, 0.62, -1.25, 1.18]], dtype=float32)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learnt_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n",
      "\n",
      "\n",
      "will 1\n",
      "[0.00 0.00 0.00 0.00 0.13 0.34 0.00 0.03 0.20 0.00 0.00 0.00 0.10 0.00 0.00 0.00 0.00 0.06 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.02 0.04 0.00\n",
      " 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.07 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00]\n",
      "['there', 'they', 'be']\n",
      "\n",
      "\n",
      "the 2\n",
      "[0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.50 0.00 0.00 0.00 0.00 0.00 0.00 0.30 0.00 0.00 0.00 0.03 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n",
      " 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.02 0.00 0.02 0.00 0.10 0.03]\n",
      "['along', 'for', 'yellow']\n",
      "\n",
      "\n",
      "and 3\n",
      "[0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.58 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.09 0.07 0.00 0.00 0.00 0.00 0.04 0.05 0.00 0.07 0.10 0.00 0.00 0.00 0.00 0.00\n",
      " 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00]\n",
      "['murder', 'revisions', 'indeed']\n",
      "\n",
      "\n",
      "there 4\n",
      "[0.00 0.87 0.00 0.00 0.00 0.00 0.03 0.00 0.00 0.00 0.09 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n",
      " 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00]\n",
      "['time', 'indeed', 'will']\n",
      "\n",
      "\n",
      "be 5\n",
      "[0.00 0.65 0.00 0.00 0.00 0.00 0.35 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n",
      " 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00]\n",
      "['an', 'time', 'will']\n",
      "\n",
      "\n",
      "time 6\n",
      "[0.00 0.00 0.00 0.00 0.01 0.80 0.00 0.00 0.00 0.00 0.00 0.18 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.00\n",
      " 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00]\n",
      "['there', 'to', 'be']\n",
      "\n",
      "\n",
      "that 7\n",
      "[0.00 0.23 0.00 0.00 0.00 0.00 0.00 0.00 0.11 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.03 0.00 0.00 0.00 0.23 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n",
      " 0.00 0.00 0.00 0.15 0.00 0.00 0.08 0.00 0.00 0.14 0.00 0.00 0.00 0.00 0.01 0.00 0.00]\n",
      "['one', 'will', 'smoke']\n",
      "\n",
      "\n",
      "they 8\n",
      "[0.00 0.93 0.00 0.00 0.00 0.00 0.00 0.05 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n",
      " 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.00]\n",
      "['muzzle', 'that', 'will']\n",
      "\n",
      "\n",
      "yellow 9\n",
      "[0.00 0.00 0.61 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.31 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.00\n",
      " 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.07 0.00 0.00 0.00 0.00 0.00 0.00 0.00]\n",
      "['fog', 'smoke', 'the']\n",
      "\n",
      "\n",
      "indeed 10\n",
      "[0.00 0.00 0.00 0.61 0.37 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n",
      " 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00]\n",
      "['be', 'there', 'and']\n",
      "\n",
      "\n",
      "to 11\n",
      "[0.00 0.01 0.00 0.00 0.00 0.00 0.23 0.00 0.01 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.00 0.00 0.28 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n",
      " 0.00 0.04 0.00 0.01 0.00 0.00 0.01 0.23 0.14 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00]\n",
      "['sing', 'time', 'murder']\n",
      "\n",
      "\n",
      "say 12\n",
      "[0.00 0.84 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.10 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.04 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n",
      " 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.00]\n",
      "['but', 'how', 'will']\n",
      "\n",
      "\n",
      "how 13\n",
      "[0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.88 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.07 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n",
      " 0.00 0.00 0.02 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00]\n",
      "['lord', 'but', 'his']\n",
      "\n",
      "\n",
      "his 14\n",
      "[0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.86 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.05 0.00 0.00 0.00 0.08 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n",
      " 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00]\n",
      "['hair', 'arms', 'how']\n",
      "\n",
      "\n",
      "thin 15\n",
      "[0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.80 0.00 0.00 0.00 0.14 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n",
      " 0.01 0.00 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00]\n",
      "['an', 'are', 'growing']\n",
      "\n",
      "\n",
      "for 16\n",
      "[0.00 0.00 0.49 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.49 0.00 0.00 0.00 0.00 0.00 0.00\n",
      " 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00]\n",
      "['that', 'decisions', 'the']\n",
      "\n",
      "\n",
      "do 17\n",
      "[0.00 0.95 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n",
      " 0.00 0.00 0.00 0.00 0.01 0.02 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00]\n",
      "['i', 'not', 'will']\n",
      "\n",
      "\n",
      "rubs 18\n",
      "[0.00 0.01 0.00 0.00 0.00 0.00 0.00 0.35 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.63 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n",
      " 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00]\n",
      "['will', 'that', 'its']\n",
      "\n",
      "\n",
      "its 19\n",
      "[0.00 0.00 0.01 0.00 0.00 0.00 0.00 0.01 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.13 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n",
      " 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.24 0.00 0.59 0.00 0.00 0.00 0.00]\n",
      "['rubs', 'back', 'muzzle']\n",
      "\n",
      "\n",
      "window 20\n",
      "[0.00 0.00 0.74 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.23 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n",
      " 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.00]\n",
      "['muzzle', 'panes', 'the']\n",
      "\n",
      "\n",
      "panes 21\n",
      "[0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.98 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n",
      " 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00]\n",
      "['along', 'is', 'window']\n",
      "\n",
      "\n",
      "smoke 22\n",
      "[0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.41 0.00 0.56 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n",
      " 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.02 0.00]\n",
      "['along', 'that', 'yellow']\n",
      "\n",
      "\n",
      "murder 23\n",
      "[0.00 0.00 0.00 0.58 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.40 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n",
      " 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00]\n",
      "['an', 'to', 'and']\n",
      "\n",
      "\n",
      "create 24\n",
      "[0.00 0.00 0.00 0.98 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n",
      " 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00]\n",
      "['are', 'an', 'and']\n",
      "\n",
      "\n",
      "hair 25\n",
      "[0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.01 0.00 0.00 0.00 0.51 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.17 0.01 0.00 0.00 0.10 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n",
      " 0.07 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.08 0.00]\n",
      "['legs', 'is', 'his']\n",
      "\n",
      "\n",
      "is 26\n",
      "[0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.16 0.02 0.68 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.02\n",
      " 0.05 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00]\n",
      "['an', 'hair', 'growing']\n",
      "\n",
      "\n",
      "growing 27\n",
      "[0.00 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.02 0.00 0.00 0.00 0.00 0.00 0.13 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.00 0.00 0.33 0.00 0.00 0.01 0.16 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n",
      " 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.03 0.00 0.00 0.00 0.00 0.28 0.00]\n",
      "['legs', 'along', 'is']\n",
      "\n",
      "\n",
      "but 28\n",
      "[0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.52 0.44 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n",
      " 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00]\n",
      "['do', 'how', 'say']\n",
      "\n",
      "\n",
      "arms 29\n",
      "[0.00 0.00 0.00 0.60 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.18 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.04 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n",
      " 0.15 0.00 0.02 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00]\n",
      "['an', 'his', 'and']\n",
      "\n",
      "\n",
      "legs 30\n",
      "[0.00 0.00 0.00 0.72 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.12 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n",
      " 0.11 0.00 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00]\n",
      "['an', 'are', 'and']\n",
      "\n",
      "\n",
      "are 31\n",
      "[0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.12 0.00 0.00 0.00 0.00 0.11 0.00 0.00 0.00 0.00 0.00 0.21 0.00 0.00 0.00 0.34 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n",
      " 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.19 0.00]\n",
      "['along', 'is', 'legs']\n",
      "\n",
      "\n",
      "decisions 32\n",
      "[0.00 0.00 0.00 0.47 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.00 0.51 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n",
      " 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00]\n",
      "['to', 'and', 'for']\n",
      "\n",
      "\n",
      "revisions 33\n",
      "[0.00 0.01 0.01 0.24 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.48 0.00 0.00 0.00 0.00 0.03 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.19 0.00 0.00 0.00 0.00\n",
      " 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.00 0.00]\n",
      "['which', 'and', 'to']\n",
      "\n",
      "\n",
      "which 34\n",
      "[0.00 0.01 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.04 0.00 0.04 0.00 0.00 0.00 0.00 0.03 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.05 0.51 0.00 0.23 0.00 0.00 0.00\n",
      " 0.00 0.00 0.00 0.00 0.00 0.00 0.03 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00]\n",
      "['decisions', 'a', 'revisions']\n",
      "\n",
      "\n",
      "a 35\n",
      "[0.00 0.00 0.03 0.00 0.00 0.05 0.00 0.03 0.00 0.00 0.00 0.02 0.01 0.06 0.00 0.00 0.00 0.03 0.00 0.01 0.00 0.00 0.04 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.38 0.00 0.16 0.00 0.00\n",
      " 0.00 0.00 0.00 0.00 0.00 0.06 0.00 0.00 0.00 0.04 0.01 0.01 0.00 0.01 0.00 0.02 0.00]\n",
      "['not', 'minute', 'which']\n",
      "\n",
      "\n",
      "minute 36\n",
      "[0.00 0.83 0.01 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.02 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.02 0.01 0.00 0.08 0.00 0.00 0.00\n",
      " 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00]\n",
      "['indeed', 'a', 'will']\n",
      "\n",
      "\n",
      "reverse 37\n",
      "[0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n",
      " 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00]\n",
      "['and', 'time', 'will']\n",
      "\n",
      "\n",
      "am 38\n",
      "[0.00 0.01 0.00 0.08 0.00 0.00 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.00 0.00 0.04 0.09 0.00 0.00 0.01 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n",
      " 0.67 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.02 0.00 0.00 0.01 0.00]\n",
      "['and', 'growing', 'an']\n",
      "\n",
      "\n",
      "an 39\n",
      "[0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.03 0.00 0.00 0.00 0.00 0.00 0.06 0.01 0.00 0.00 0.00 0.00 0.12 0.01 0.19 0.00 0.00 0.00 0.01 0.00 0.00 0.01 0.00 0.00 0.00 0.12\n",
      " 0.00 0.16 0.00 0.01 0.00 0.00 0.00 0.02 0.02 0.00 0.00 0.12 0.00 0.07 0.00 0.01 0.00]\n",
      "['upon', 'attendant', 'growing']\n",
      "\n",
      "\n",
      "attendant 40\n",
      "[0.00 0.00 0.00 0.08 0.00 0.00 0.02 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.04 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.11 0.01 0.00 0.00 0.27 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n",
      " 0.17 0.01 0.23 0.00 0.00 0.00 0.00 0.01 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00]\n",
      "['an', 'lord', 'are']\n",
      "\n",
      "\n",
      "lord 41\n",
      "[0.00 0.02 0.00 0.01 0.01 0.00 0.01 0.00 0.01 0.00 0.00 0.00 0.01 0.03 0.00 0.00 0.00 0.01 0.00 0.00 0.02 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.01\n",
      " 0.00 0.27 0.00 0.35 0.00 0.00 0.11 0.04 0.04 0.01 0.00 0.01 0.00 0.00 0.00 0.00 0.00]\n",
      "['think', 'attendant', 'one']\n",
      "\n",
      "\n",
      "one 42\n",
      "[0.00 0.01 0.00 0.00 0.00 0.00 0.00 0.49 0.00 0.00 0.00 0.01 0.00 0.00 0.01 0.00 0.00 0.01 0.00 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.04 0.00 0.00 0.00 0.02 0.00 0.00 0.00 0.00 0.00 0.00\n",
      " 0.00 0.00 0.39 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00]\n",
      "['but', 'lord', 'that']\n",
      "\n",
      "\n",
      "i 43\n",
      "[0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.06 0.01 0.00 0.00 0.00 0.78 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.02 0.01 0.00 0.00 0.02 0.00 0.00\n",
      " 0.00 0.00 0.01 0.00 0.00 0.00 0.02 0.04 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00]\n",
      "['sing', 'say', 'do']\n",
      "\n",
      "\n",
      "not 44\n",
      "[0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.02 0.00 0.00 0.00 0.09 0.01 0.00 0.00 0.00 0.63 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.05 0.00 0.00 0.01 0.00 0.00\n",
      " 0.00 0.01 0.00 0.01 0.00 0.00 0.09 0.04 0.00 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00]\n",
      "['think', 'say', 'do']\n",
      "\n",
      "\n",
      "think 45\n",
      "[0.00 0.01 0.00 0.00 0.00 0.00 0.00 0.87 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.06 0.00 0.00 0.00 0.01 0.00 0.00\n",
      " 0.00 0.00 0.00 0.00 0.00 0.03 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00]\n",
      "['not', 'decisions', 'that']\n",
      "\n",
      "\n",
      "sing 46\n",
      "[0.00 0.89 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.08 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n",
      " 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00]\n",
      "['not', 'to', 'will']\n",
      "\n",
      "\n",
      "me 47\n",
      "[0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.96 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.00\n",
      " 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00]\n",
      "['be', 'which', 'to']\n",
      "\n",
      "\n",
      "fog 48\n",
      "[0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.23 0.00 0.73 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n",
      " 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.03 0.00]\n",
      "['along', 'that', 'yellow']\n",
      "\n",
      "\n",
      "back 49\n",
      "[0.00 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.04 0.00 0.00 0.00 0.00 0.00 0.39 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.02 0.00 0.00 0.00 0.00 0.00 0.00 0.02 0.00 0.00 0.00 0.03\n",
      " 0.00 0.07 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.25 0.00 0.13 0.00 0.00 0.00]\n",
      "['on', 'upon', 'its']\n",
      "\n",
      "\n",
      "upon 50\n",
      "[0.00 0.00 0.68 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.00\n",
      " 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.22 0.00 0.04 0.00 0.00 0.00 0.00]\n",
      "['muzzle', 'back', 'the']\n",
      "\n",
      "\n",
      "muzzle 51\n",
      "[0.00 0.09 0.00 0.00 0.00 0.00 0.00 0.02 0.00 0.00 0.00 0.01 0.00 0.01 0.00 0.00 0.00 0.00 0.00 0.70 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.01\n",
      " 0.00 0.01 0.01 0.00 0.00 0.01 0.00 0.00 0.00 0.00 0.00 0.04 0.00 0.05 0.00 0.00 0.00]\n",
      "['on', 'will', 'its']\n",
      "\n",
      "\n",
      "on 52\n",
      "[0.00 0.00 0.71 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.03 0.00 0.00 0.00 0.00 0.00 0.00\n",
      " 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.22 0.00 0.00 0.00 0.00]\n",
      "['decisions', 'muzzle', 'the']\n",
      "\n",
      "\n",
      "slides 53\n",
      "[0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.80 0.00 0.05 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n",
      " 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.00 0.10 0.00]\n",
      "['yellow', 'along', 'that']\n",
      "\n",
      "\n",
      "along 54\n",
      "[0.00 0.00 0.70 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.01 0.04 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.00 0.00 0.00\n",
      " 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.03 0.02 0.00 0.13 0.00 0.02 0.00 0.00]\n",
      "['smoke', 'muzzle', 'the']\n",
      "\n",
      "\n",
      "street 55\n",
      "[0.00 0.00 0.97 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.00 0.00 0.00\n",
      " 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.00]\n",
      "['muzzle', 'decisions', 'the']\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess,model_path)\n",
    "    top_n_words = 3\n",
    "    for word_no in range(1,vocabulary_size):\n",
    "        word = data_obj.inverse_tokenizer(word_no)\n",
    "        feed_dict={train_inputs:np.array([word_no])}\n",
    "        word_embed, word_pred = sess.run([embed,soft_max],feed_dict)\n",
    "        word_pred = word_pred.squeeze()\n",
    "        top_n_args = word_pred.argsort()[-top_n_words:]\n",
    "        print(\"\\n\")\n",
    "        print(word,word_no)\n",
    "        print(word_pred)\n",
    "        print(data_obj.inverse_tokenizer_sentence(top_n_args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>and indeed there will be time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>there will be time  there will be time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>there will be time to murder and create</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>and indeed there will be time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>they will say   how his hair is growing thin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>they will say   but how his arms and legs are thin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>for decisions and revisions which a minute will reverse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>am an attendant lord  one that will do</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>i do not think that they will sing to me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>the yellow fog that rubs its back upon the window panes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>the yellow smoke that rubs its muzzle on the window panes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>for the yellow smoke that slides along the street</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                               0\n",
       "0                                 and indeed there will be time \n",
       "1                        there will be time  there will be time \n",
       "2                      there will be time to murder and create  \n",
       "3                                 and indeed there will be time \n",
       "4               they will say   how his hair is growing thin    \n",
       "5         they will say   but how his arms and legs are thin    \n",
       "6      for decisions and revisions which a minute will reverse  \n",
       "7                        am an attendant lord  one that will do \n",
       "8                     i do not think that they will sing to me  \n",
       "9      the yellow fog that rubs its back upon the window panes  \n",
       "10   the yellow smoke that rubs its muzzle on the window panes  \n",
       "11           for the yellow smoke that slides along the street  "
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
